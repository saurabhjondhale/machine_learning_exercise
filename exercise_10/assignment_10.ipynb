{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use the already imported library and the Python standard library. For the evaluation you may also use scikit-learn (`sklearn`) and `matplotlib`. Make sure that the dataset `Churn_Modelling.csv` is in the same directory as the notebook.\n",
    "\n",
    "List your team members (name and immatriculation number) and indicate whether you are a B.Sc. Data Science or other group in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> *Write*\n",
    "* Saurabh J\n",
    "* 3574931\n",
    "* MSc Info-Tech\n",
    "\n",
    "* Erick Villanueva\n",
    "* 3731779\n",
    "* Computational Linguistics MSc.\n",
    "\n",
    "* Song Cheng\n",
    "* 3627396\n",
    "* Autonome Systeme MSc\n",
    "\n",
    "*of all assignment group participants here. (double klick here to edit)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Feedforward Neural Network: Programming (40 Points)\n",
    "\n",
    "In this task, you will implement a feedforward neural network for binary classification. The hyperparameters of the model are:\n",
    "- `input_size`: The dimension of the input vector.\n",
    "- `output_size`: The dimension of the output vector, which is 1 in this case.\n",
    "- `hidden_layers`: A list of integers where each integer represents the number of neurons in that hidden layer.\n",
    "- `learning_rate`: The learning rate for gradient descent.\n",
    "- `epochs`: The number of epochs/iterations performed during training.\n",
    "\n",
    "B.Sc. Data Science only have to implement for a single hidden layer. All other students have to implement the network for any length of hidden_layers.\n",
    "\n",
    "The activation function for every layer is sigmoid function.\n",
    "\n",
    "You have to implement the `FeedforwardNeuralNetworkClassifier`.\n",
    "\n",
    "The `fit` method trains the network.\n",
    "Use backpropagation with gradient descent.\n",
    "Use the whole training data set for each training epoch.\n",
    "Use the mean squared error as loss function.\n",
    "\n",
    "The `predict` method computes the forward-pass of the network.\n",
    "\n",
    "Evaluate your classifier on the test data with the mean squared error. Try out different hyper-parameters and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, hidden_layers, output_size=1, learning_rate=0.01, epochs=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.rand(layer_sizes[i], layer_sizes[i+1]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "        \n",
    "    def sigmoid_derivative(self, x):\n",
    "        sig = self.sigmoid(x)\n",
    "        return sig * (1.0 - sig)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward pass\n",
    "            activations, zs = self.forward(X_train)\n",
    "            y_pred = activations[-1]\n",
    "\n",
    "            # Compute loss\n",
    "            current_loss = np.mean(y_pred - y_train)\n",
    "\n",
    "            # Backward pass to compute gradients\n",
    "            nabla_w, nabla_b = self.backward(X_train, y_train, activations, zs)\n",
    "\n",
    "            # Update weights and biases\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.lr * nabla_w[i]\n",
    "                self.biases[i] -= self.lr * nabla_b[i]\n",
    "\n",
    "            # Print epoch and loss\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {current_loss}\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            z = activations[-1] @ self.weights[i] + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activations.append(self.sigmoid(z))\n",
    "        \n",
    "        return activations, zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical data\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])\n",
    "\n",
    "# One Hot Encoding the \"Geography\" column\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 8000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      6\u001b[0m nn \u001b[38;5;241m=\u001b[39m FeedforwardNeuralNetworkClassifier(input_size\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_layers\u001b[38;5;241m=\u001b[39mhidden_layers, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m, in \u001b[0;36mFeedforwardNeuralNetworkClassifier.train\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     55\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_pred \u001b[38;5;241m-\u001b[39m y_train)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Backward pass to compute gradients\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m nabla_w, nabla_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Update weights and biases\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m, in \u001b[0;36mFeedforwardNeuralNetworkClassifier.backward\u001b[0;34m(self, X, y_true, activations, zs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     41\u001b[0m     sp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_derivative(zs[\u001b[38;5;241m-\u001b[39mi])\n\u001b[0;32m---> 42\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m \u001b[38;5;241m*\u001b[39m sp\n\u001b[1;32m     43\u001b[0m     nabla_w[\u001b[38;5;241m-\u001b[39mi] \u001b[38;5;241m=\u001b[39m activations[\u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m delta\n\u001b[1;32m     44\u001b[0m     nabla_b[\u001b[38;5;241m-\u001b[39mi] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 8000)"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "hidden_layers = [32, 16, 8]  # Example hidden layers with 10 neurons each\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "nn = FeedforwardNeuralNetworkClassifier(input_size=X_train.shape[1], hidden_layers=hidden_layers, learning_rate=learning_rate, epochs=epochs)\n",
    "nn.train(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "y_pred = nn.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test.reshape(-1, 1))\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot the training error over epochs\n",
    "plt.plot(nn.training_errors)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training Error over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el_kg_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
